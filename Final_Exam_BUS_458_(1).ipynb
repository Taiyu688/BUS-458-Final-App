{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Final Exam BUS 458\n",
        "Taiyu Zhang (200429688) ShuiTian Lian (200248618)\n",
        "\n",
        "My partner and I collaborated on this task. For each question, we discussed different approaches together and then decided on what we believed was the most appropriate solution."
      ],
      "metadata": {
        "id": "KiadP5p606ye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Exam BUS 458 — Loan Data Analysis\n",
        "**Objective:** Your objective is to identify the key drivers of loan approval and recommend which lender should be prioritized for each customer to maximize total payout\n",
        "\n",
        "### **Instructions**\n",
        "- Follow the prompts in each section of this notebook.\n",
        "- **Where you see a “Question,” answer it directly below in a Markdown (text) cell — not as a code comment.**\n",
        "- Your **reasoning, interpretations, and insights** should be written in text cells, clearly separated from code.\n",
        "- You are encouraged to add extra **code cells**, **visualizations**, or **short explanations** if they strengthen your analysis or help you justify decisions.\n",
        "- Make sure your notebook runs cleanly from start to finish without errors.\n"
      ],
      "metadata": {
        "id": "gsXFUVo5Je9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "PaorYd2r_OgJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data (via Google Drive or direct upload)"
      ],
      "metadata": {
        "id": "NIvHBm0TKiO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your code here\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/sample_data/loan_data_analysis_final.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "jcJpsp8QacX5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "05952eda-c57c-4b92-af49-a37d24169a44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_data/loan_data_analysis_final.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3086686890.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Add your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample_data/loan_data_analysis_final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/loan_data_analysis_final.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check first 5 rows of dataframe\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "rykBy6BDPsu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns that have no variation or are unique\n",
        "\n",
        "cols_to_drop = []\n",
        "\n",
        "if \"User ID\" in df.columns:\n",
        "    cols_to_drop.append(\"User ID\")\n",
        "\n",
        "for col in df.columns:\n",
        "    if df[col].nunique() == 1:\n",
        "        cols_to_drop.append(col)\n",
        "\n",
        "df = df.drop(columns=cols_to_drop)\n",
        "\n",
        "cols_to_drop"
      ],
      "metadata": {
        "id": "htsg53OyXkq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understand the Data: Get a quick overview of the dataset structure and variable meanings before preprocessing."
      ],
      "metadata": {
        "id": "TiS2pMT-OUGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your code here\n",
        "df.info()\n",
        "df.describe()\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "Z0pmgVn1PgDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preprocessing"
      ],
      "metadata": {
        "id": "JORdHAFNUpLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1.1 Handling Missing Values\n",
        "\n",
        "**Question:** Why is median imputation often preferable to mean imputation for income and FICO?\n",
        "\n",
        "\n",
        "**Answer:** Median imputation is not significantly affected by extreme cases such as minimum and maximum values. Both income and FICO data exhibit extreme cases and have many missing values."
      ],
      "metadata": {
        "id": "1x6z7up--ubS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect missing values\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "bTJvNndpU1bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply imputation as instructed in the instructions pdf (create copies so original rows can be restored if needed)\n",
        "# Impute\n",
        "\n",
        "df_imputed = df.copy()\n",
        "\n",
        "if 'FICO_score' in df_imputed.columns:\n",
        "    df_imputed['FICO_score']= df_imputed['FICO_score'].fillna(df_imputed['FICO_score'].median())\n",
        "\n",
        "if 'Monthly_Gross_Income' in df_imputed.columns:\n",
        "    df_imputed['Monthly_Gross_Income']= df_imputed['Monthly_Gross_Income'].fillna(df_imputed['Monthly_Gross_Income'].median())\n",
        "\n",
        "if 'Employment_Sector' in df_imputed.columns:\n",
        "    df_imputed['Employment_Sector']= df_imputed['Employment_Sector'].fillna('Unknow')\n",
        "df_imputed.isna().sum()"
      ],
      "metadata": {
        "id": "_dF9eEltV-Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Outliers Detection and Removal\n",
        "\n",
        "**Question:**\n",
        "Which features had the most outliers, and what impact could they have on the model?\n",
        "\n",
        "The Grant_Loan_Amount and Requested_Loan_Amnount had the most outlier."
      ],
      "metadata": {
        "id": "iy_Fhc1uio3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numeric_cols = ['Granted_Loan_Amount', 'Requested_Loan_Amount', 'FICO_score', 'Monthly_Gross_Income', 'Monthly_Housing_Payment']\n",
        "\n",
        "# Boxplots to highlight outliers for numerical columns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numeric_cols = ['Granted_Loan_Amount', 'Requested_Loan_Amount', 'FICO_score', 'Monthly_Gross_Income', 'Monthly_Housing_Payment']\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "df_imputed [numeric_cols].boxplot()\n",
        "plt.ylabel(\"Value\")\n",
        "plt.title(\"Boxplots of numeric features\")\n",
        "\n",
        "plt.xticks(rotation=45)\n"
      ],
      "metadata": {
        "id": "uulkaWolk6xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Outliers using Z-score\n",
        "\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# Compute Z-scores for only numeric columns\n",
        "z_scores = np.abs(stats.zscore(df[numeric_cols], nan_policy='omit'))\n",
        "\n",
        "# Choose threshold\n",
        "threshold = 3  # common choice: 3 standard deviations\n",
        "\n",
        "# Identify rows to keep (all Z-scores <= threshold)\n",
        "rows_to_keep = (z_scores < threshold).all(axis=1)\n",
        "\n",
        "# Track counts before/after\n",
        "before_count = df.shape[0]\n",
        "df_clean = df[rows_to_keep].copy()\n",
        "after_count = df_clean.shape[0]\n",
        "\n",
        "print(f\"Outlier removal complete:\")\n",
        "print(f\"Rows before: {before_count}\")\n",
        "print(f\"Rows after:  {after_count}\")\n",
        "print(f\"Rows removed: {before_count - after_count}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uy_gcYUCJ3r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BoxPlots after outlier removal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numeric_cols = ['Granted_Loan_Amount', 'Requested_Loan_Amount', 'FICO_score', 'Monthly_Gross_Income', 'Monthly_Housing_Payment']\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "df_clean [numeric_cols].boxplot()\n",
        "plt.ylabel(\"Value\")\n",
        "plt.title(\"Boxplots of numeric features after outlier removal\")\n",
        "\n",
        "plt.xticks(rotation=45)"
      ],
      "metadata": {
        "id": "Ejk-G8OMJ5o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "Why do we need to remove or treat outliers? Provide reasoning based on model performance or data integrity.\n",
        "\n",
        "**Answer** : The data from outliers is not representative and will affect model training.\n",
        "\n",
        "\n",
        "**Question:**\n",
        "Is there a difference between boxplots before and after outlier removal?\n",
        "\n",
        "**Answer** : Removing outliers makes the scale smlng in clearer reading of the box plt and a more intuitive understanding of the data."
      ],
      "metadata": {
        "id": "R0i7XOzsJ794"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Redundancy & Correlation Check (5 Marks)\n",
        "\n",
        "Detect duplicate data and multicollinearity.\n",
        "\n",
        "**Question:**\n",
        "Which numerical variables appear most strongly correlated with each other?\n",
        "\n",
        "**Answer:** Requested Loan Amount and Granted Loan Amount have the strongest correlation.\n",
        "\n",
        "\n",
        "\n",
        "**Question:**\n",
        "Which numerical and categorical are strongly corelated to each other?\n",
        "\n",
        "**Answer:** FICO Score and FICO SCORE Group are strongly corelated.\n"
      ],
      "metadata": {
        "id": "pAP2IqREYYjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation between numerical features\n",
        "numeric_cols = ['FICO_score', 'Monthly_Gross_Income', 'Monthly_Housing_Payment', 'Granted_Loan_Amount','Requested_Loan_Amount', 'Approved']\n",
        "\n",
        "# Plot correlation matrix, identify highly correlated pairs automatically (|corr|>0.95, excluding self-correlation)\n",
        "corr = df_clean[numeric_cols].corr()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(corr, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
        "plt.title(\"Numeric Correlation Heatmap (with strongly correlated column)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H1-61RIv-Sjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation between Numeric and Categorical Variables (Correlation Ratio / η²)\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def correlation_ratio(categories, values):\n",
        "    categories = pd.Series(categories)\n",
        "    values = pd.Series(values)\n",
        "\n",
        "    mask = categories.notna() & values.notna()\n",
        "    categories = categories[mask].astype(str).values\n",
        "    values = values[mask].astype(float).values\n",
        "\n",
        "    cat_levels = np.unique(categories)\n",
        "    overall_mean = np.mean(values)\n",
        "    numerator = sum(len(values[categories == cat]) *\n",
        "                    (np.mean(values[categories == cat]) - overall_mean) ** 2\n",
        "                    for cat in cat_levels)\n",
        "    denominator = sum((values - overall_mean) ** 2)\n",
        "    return np.sqrt(numerator / denominator) if denominator != 0 else 0\n",
        "\n",
        "# test all numeric–categorical pairs\n",
        "num_cat_results = []\n",
        "\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols_clean = df_clean.select_dtypes(include='object').columns.tolist()\n",
        "numeric_cols_clean = df_clean.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "\n",
        "numeric_cols_for_eta = [col for col in numeric_cols_clean if col not in ['bounty', 'Approved']]\n",
        "categorical_cols_for_eta = [col for col in categorical_cols_clean]\n",
        "\n",
        "\n",
        "for num_col in numeric_cols_for_eta:\n",
        "    for cat_col in categorical_cols_for_eta:\n",
        "        eta = correlation_ratio(df_clean[cat_col], df_clean[num_col])\n",
        "        num_cat_results.append((num_col, cat_col, eta))\n",
        "\n",
        "num_cat_results = sorted(num_cat_results, key=lambda x: x[2], reverse=True)\n",
        "print(\"Top 10 Numeric-Categorical Correlation Ratios (η²):\")\n",
        "for num_col, cat_col, eta in num_cat_results[:10]:\n",
        "    print(f\"{num_col} - {cat_col}: η² = {eta**2:.3f} (η = {eta:.3f})\")\n",
        "\n",
        "\n",
        "# Reorganize the correlation ratio results into a pivot table/matrix for heatmap\n",
        "eta_matrix = pd.DataFrame(num_cat_results, columns=['Numerical_Feature', 'Categorical_Feature', 'Eta'])\n",
        "eta_pivot = eta_matrix.pivot(index='Numerical_Feature', columns='Categorical_Feature', values='Eta')\n",
        "\n",
        "# Plot the heatmap of correlation ratios (Eta)\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(eta_pivot, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
        "plt.title(\"Heatmap of Correlation Ratios (η) between Numeric and Categorical Features\")\n",
        "plt.xlabel(\"Categorical Features\")\n",
        "plt.ylabel(\"Numerical Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n6OYU-G1960A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:** What features will you consider dropping after analyzing the graphs above?\n",
        "\n",
        "**Answer** : I will delate Granted Loan Amount and FICO SCORE Group.\n",
        "\n",
        "\n",
        "\n",
        "> *Note: For Decision Tree models, you do not need to remove or adjust for collinearity. The algorithm can naturally handle correlated features through its splitting mechanism. However, for Logistic Regression, multicollinearity can distort coefficient estimates and make interpretations unreliable. In that case, you may consider removing one variable from highly correlated pairs.\n"
      ],
      "metadata": {
        "id": "-LwM00vMI_9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "5pi5X4t60gox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Target Variable Distribution\n",
        "\n",
        "Analyze the distribution of the target variable (Approved) to understand class balance in loan approval outcomes.\n",
        "\n",
        "\n",
        "**Question:**\n",
        "Is the dataset balanced or imbalanced? Briefly explain the implication for modeling.\n",
        "\n",
        "\n",
        "**Answer:**\n",
        "This dataset is imbalanced. There are more cases with Approved = 0 than Approved = 1. It will tent predict Approved = 0."
      ],
      "metadata": {
        "id": "O8RERDMA0kTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 'Approved' to visualise the count and balance\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(4, 3))\n",
        "df_clean['Approved'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Distribution of Approved status\")\n",
        "plt.xlabel(\"Approved\")\n",
        "plt.ylabel(\"Count\")\n"
      ],
      "metadata": {
        "id": "dHV-4EH5b3Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Distribution of Numerical Variables by Approval\n",
        "\n",
        "Explore how key numerical variables differ between approved and rejected loan applications.\n",
        "\n",
        "\n",
        "**Question:**\n",
        "Which numerical variable is the most helpful variable ( most crucial predictor) ?\n",
        "\n",
        "**Answer:**\n",
        "According these graphs I will say the most helpful variale is FICO SCORE.\n"
      ],
      "metadata": {
        "id": "5dooFRhP1Vp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise numerical variables against target variable\n",
        "\n",
        "numerical_cols = ['Granted_Loan_Amount', 'FICO_score', 'Monthly_Gross_Income', 'Monthly_Housing_Payment']\n",
        "\n",
        "for col in numerical_cols:\n",
        "    plt.figure(figsize=(12,5))\n",
        "    sns.histplot(\n",
        "        data=df,\n",
        "        x=col,\n",
        "        hue='Approved',\n",
        "        kde=True,\n",
        "        stat='density',\n",
        "        common_norm=False,\n",
        "        palette='coolwarm',\n",
        "        alpha=0.6\n",
        "    )\n",
        "    plt.title(f'Distribution of {col} by Approval Status')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Density')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.boxplot(\n",
        "        data=df,\n",
        "        x='Approved',\n",
        "        y=col,\n",
        "        hue='Approved',\n",
        "        palette='coolwarm',\n",
        "        legend=False\n",
        "    )\n",
        "    plt.title(f'Box Plot of {col} by Approval Status')\n",
        "    plt.xlabel('Approved (0 = Denied, 1 = Approved)')\n",
        "    plt.ylabel(col)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "0SxbCB_H1UDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Categorical Variables and Approval\n",
        "\n",
        "Analyze how categorical variables (such as Employment_Sector, Loan_Purpose, or Residence_Type) relate to the loan approval outcome.\n",
        "\n",
        "\n",
        "**Question:**\n",
        "Which categorical variable appears to have the strongest relationship with loan approval, and how can you tell?\n",
        "\n",
        "**Answer:**\n",
        "The strongest relationship to loan approval FICO score group. The highest approval rate is for the excellent level, the lowest approval rate is for poor group.\n",
        "\n",
        "**Question:**\n",
        "Are there any categories (e.g., Reason, Employment_Status) that appear to have minimal predictive value? Justify your answer with approval rate differences.\n",
        "\n",
        "**Answer:**\n",
        "Reason has extremely low predictive value. Regardlesss of the reason, their approval rates differ by the 10-12%, and this variation has little impact on the approval rate.  \n"
      ],
      "metadata": {
        "id": "kznt3YXa2aOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise categorical variables against target variable\n",
        "\n",
        "categorical_cols = [\n",
        "    'Reason', 'Fico_Score_group', 'Employment_Status',\n",
        "    'Employment_Sector', 'Lender', 'Ever_Bankrupt_or_Foreclose'\n",
        "]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    approval_rates = df.groupby(col)['Approved'].mean().sort_values(ascending=False) * 100\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.barplot(x=approval_rates.index, y=approval_rates.values, color='skyblue')\n",
        "    plt.title(f'Approval Rate by {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Approval Rate (%)')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nApproval Rate by {col}:\\n{approval_rates.round(2)}\")\n"
      ],
      "metadata": {
        "id": "0QGjRaAFEKti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross Feature plots for multi-dimensional analysis."
      ],
      "metadata": {
        "id": "IwRMVnb3K6o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fico score and income\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.scatterplot(data=df,x='FICO_score',y='Monthly_Gross_Income')\n",
        "plt.title('FICO Score vs Monthly Gross Income by Approval')\n",
        "plt.xlabel('FICO_score')\n",
        "plt.ylabel('Monthly_Gross_Income')\n",
        "\n",
        "\n",
        "# Monthly income vs Granted Loan Amount\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.scatterplot(data=df,x='Monthly_Gross_Income',y='Granted_Loan_Amount')\n",
        "\n",
        "plt.title('Monthly Gross Income vs Granted Loan Amount by Approval')\n",
        "plt.xlabel('Monthly_Gross_Income')\n",
        "plt.ylabel('Granted_Loan_Amount')\n",
        "\n",
        "\n",
        "# Fico score vs Granted Loan Amount\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.scatterplot(data=df, x='FICO_score', y='Granted_Loan_Amount')\n",
        "plt.title('FICO Score vs Granted Loan Amount by Lender')\n",
        "plt.xlabel('FICO_score')\n",
        "plt.ylabel('Granted_Loan_Amount')\n"
      ],
      "metadata": {
        "id": "0d8rD-LULLfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Business Insights from EDA\n",
        "\n",
        "After completing your visualizations and cross-feature analyses, summarize your findings in the context of loan approval and lender matching.  \n",
        "Use this section to connect data patterns to real-world meaning before building predictive models.\n",
        "\n",
        "\n",
        "**Question:** Which variables are the most helpful in understanding if a customer is going to be approved or denied for a loan?  \n",
        "\n",
        "**Answer**: FICO score, Employemnet Status, Lender, Income, FICO core group.\n",
        "\n",
        "**Question:** Are there any feature modifications or transformations that would improve the predictive power of a variable?  \n",
        "\n",
        "**Answer**:\n",
        "\n",
        "I will add the percentage of the monthly income spent on housing payment. It can improve the prefictive power. Different expendtures bring differrent risk for people with differnt income.\n",
        "\n",
        "**Question:** What is each lender’s average approval rate?  \n",
        "\n",
        "**Answer**:\n",
        "Lender's each approval rate is\n",
        "C    17.18%\n",
        "A    10.91%\n",
        "B     7.37%.\n",
        "\n",
        "**Question:** Are there any clear differences between the three lenders in terms of which types of customers they approve?  \n",
        "\n",
        "**Answer**: I just determine the differnece in approval rate between different lenders. However,I cannot make sure which types customers are approved.\n",
        "\n",
        "**Question:** Are there variables that reliably predict a customer’s approval likelihood for a particular lender?  \n",
        "\n",
        "**Answer**: I think income monthly, employment status, and monthly income percentage of specific expenditures are very powerful variables.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Treat this section as your interpretation bridge between EDA and modeling. The goal is to show that you understand *why* certain patterns exist, not just that they exist.\n"
      ],
      "metadata": {
        "id": "jcYPIS4mLb6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Training"
      ],
      "metadata": {
        "id": "6RgjVZ7tKMiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Train/Test Split and Encoding"
      ],
      "metadata": {
        "id": "NzrBHcu07bOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create df_model from df_clean for preprocessing\n",
        "df_model = df_clean.copy()\n",
        "\n",
        "# --- Impute all remaining NaNs in df_model ---\n",
        "# First, handle numerical columns\n",
        "for col in df_model.select_dtypes(include=np.number).columns:\n",
        "    if df_model[col].isnull().any():\n",
        "        df_model[col] = df_model[col].fillna(df_model[col].median())\n",
        "\n",
        "# Then, handle categorical/object columns\n",
        "for col in df_model.select_dtypes(include='object').columns:\n",
        "    if df_model[col].isnull().any():\n",
        "        df_model[col] = df_model[col].fillna('Unknown_Category')\n",
        "\n",
        "# Ensure 'Ever_Bankrupt_or_Foreclose' is treated as categorical for get_dummies if it has few unique values\n",
        "if 'Ever_Bankrupt_or_Foreclose' in df_model.columns:\n",
        "    df_model['Ever_Bankrupt_or_Foreclose'] = df_model['Ever_Bankrupt_or_Foreclose'].astype(str)\n",
        "\n",
        "# Define target variable and features\n",
        "y = df_model['Approved']\n",
        "# Drop target variable and 'bounty' (if not used as feature)\n",
        "X = df_model.drop(columns=['Approved', 'bounty'])\n",
        "\n",
        "# Define categorical features to encode\n",
        "# These columns should now be free of NaNs thanks to the imputation above\n",
        "cat_feats = [\n",
        "    'Reason',\n",
        "    'Fico_Score_group',\n",
        "    'Employment_Status',\n",
        "    'Employment_Sector',\n",
        "    'Lender',\n",
        "    'Ever_Bankrupt_or_Foreclose' # Now string type\n",
        "]\n",
        "\n",
        "# One-hot encode categorical features\n",
        "X = pd.get_dummies(X, columns=cat_feats, drop_first=True)\n",
        "\n",
        "# Set RANDOM_STATE = 42 for reproducibility.\n",
        "random_state = 42\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)"
      ],
      "metadata": {
        "id": "JACbbSYRKRgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Logistic Regression Model\n",
        "Build and interpret a Logistic Regression model to predict the likelihood of loan approval (Approved = 1).\n",
        "\n",
        "**Question**\n",
        "How accurate is the model on the test data, and what does this imply about its predictive strength?\n",
        "\n",
        "**Answer**\n",
        "The accurate was around 71.48%."
      ],
      "metadata": {
        "id": "7uM-S-GW7qMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "logmodel = LogisticRegression(class_weight='balanced',random_state=42,solver='liblinear')\n",
        "logmodel.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "xUIM9snc7m4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "log_predictions = logmodel.predict(X_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, log_predictions)\n",
        "print(\"Test Accuracy:\", test_accuracy )\n",
        "\n",
        "print(\"\\nLogistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, log_predictions))\n",
        "\n",
        "print(\"\\nLogistic Regression Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, log_predictions))"
      ],
      "metadata": {
        "id": "hdgrwrVpRq02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Decision Tree Classifier Model\n",
        "\n",
        "Build and interpret a Decision Tree Classifier Model to predict the likelihood of loan approval (Approved = 1).\n",
        "\n",
        "\n",
        "\n",
        "**Question**\n",
        "How does the Decision Tree’s accuracy and AUC compare to the Logistic Regression model?\n",
        "\n",
        "**Answer**\n",
        "Add your answer here\n"
      ],
      "metadata": {
        "id": "S4JZOI2vmi-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Train Decision Tree model\n",
        "\n",
        "dtree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=15,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    max_features=None,\n",
        "    class_weight='balanced',\n",
        "    random_state=random_state)\n",
        "\n",
        "dtree.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Kz0WbxOd70i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "tree_predictions = dtree.predict(X_test)\n",
        "tree_proba = dtree.predict_proba(X_test)[:, 1]\n",
        "tree_accuracy = accuracy_score(y_test, tree_predictions)\n",
        "print(\"Decision Tree Test Accuracy:\", tree_accuracy)\n",
        "\n",
        "print(\"\\nDecision Tree Classification Report:\")\n",
        "print(classification_report(y_test, tree_predictions))\n",
        "\n",
        "print(\"\\nDecision Tree Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, tree_predictions))\n",
        "\n",
        "tree_auc = roc_auc_score(y_test, tree_proba)\n",
        "print(\"\\nDecision Tree ROC-AUC:\", tree_auc)"
      ],
      "metadata": {
        "id": "-Tu6kDr6RxoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable importance for Decision Tree\n",
        "# Print top features\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "feat_importances = pd.Series(dtree.feature_importances_, index=X_train.columns)\n",
        "feat_importances = feat_importances.sort_values(ascending=False)\n",
        "print(\"Top 10 Important Features (Decision Tree):\")\n",
        "print(feat_importances.head(10).round(4))"
      ],
      "metadata": {
        "id": "4C7HApQMDDde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare the performance of the Logistic Regression and Decision Tree models.\n",
        "\n",
        "\n",
        "1.   Compare the performance of the Logistic Regression and Decision Tree\n",
        "models.\n",
        "2.   Evaluate and compare both models using key metrics such as ROC-AUC, Accuracy, Precision, Recall, and F1-score.\n",
        "3. Plot and interpret the confusion matrices for both models. Discuss what false positives and false negatives mean in the business context (for example, approving risky applicants vs. rejecting qualified ones).\n",
        "4. Identify which model best captures the target event (loan approval) and explain why.\n",
        "5. Justify your model choice from both technical (performance, interpretability) and business (actionability, trust, and decision-making) perspectives.\n",
        "6. Indicate the cutoff threshold you used and why it makes sense for this scenario."
      ],
      "metadata": {
        "id": "d4fzAoynn7os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = logmodel\n"
      ],
      "metadata": {
        "id": "1iRdfue79MZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your model as .pkl file for streamlit app development\n",
        "import pickle\n",
        "\n",
        "filename = 'my_model.pkl'  # Choose a path and descriptive filename with .pkl extension\n",
        "\n",
        "# Open the file in binary write mode ('wb')\n",
        "with open(filename, 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "print(f\"Model saved successfully to {filename}\")\n"
      ],
      "metadata": {
        "id": "Ju4ap5PuHJrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "Which model do you recommend and why? (Provide both technical and business reasoning.)\n",
        "\n",
        "**Answer:** I will use Logistic Regression Model, this model has the higest accuracy 71.48%. Logistic regression models can intuitively distinguish categorical variables, and in lending, they can more clearly show which variables affect the loan success rate, allowing lenders to gain a clearer understanding of their customers."
      ],
      "metadata": {
        "id": "MOusUvMWSKQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Recommendations\n",
        "In this final section, summarize your key findings and provide concise business recommendations based on your analysis.\n",
        "\n",
        "\n",
        "**Write a short paragraph including:**  \n",
        "- A brief summary of your main findings from the analysis and model comparison.  \n",
        "- The key factors that most influence loan approval.  \n",
        "- Your recommendations on data or model based on the results  (for example, how approval decisions could be improved or better understood).  \n",
        "- One next step you would take to improve the model or extend the analysis further.  \n",
        "\n",
        "\n",
        "**Add your paragraph here:**\n",
        "My analysis and model comparison show that both logistic regression and decision trees can predict loan approval results well, but logistic regression performs one percentage point better and is easier to interpret. Key factors influencing approval include credit quality FICO score/FICO score group, monthly income, housing payment, and whether there are any bankruptcy issues. Based on these results, I recommend using the logistic regression model as the primary decision support tool so that they can clearly understand which factors increase or decrease the probability of approval. Next, I can collect more data, add more detailed variables, and retrain the model to see if they can improve accuracy.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Question:**  What trade-offs or risks should the company be aware of when using this model?  \n",
        "\n",
        "**Answer:** The model's accuracy is not high enough, so we cannot rely entirely on it. Loan decisions are influenced by the economic environment, and these factors cannot be predicted by the model. Therefore, the company needs to consider external factors such as the economic environment when making decisions; the model can only provide a reference.\n",
        "\n",
        "**Question:** How confident are you in your model’s generalizability — what might change its performance in real-world use?   \n",
        "\n",
        "**Answer:**\n",
        "I have confidence in the model, but I can't rely solely on it. Overfitting can affect model performance.\n",
        "\n",
        "**Question:** If you had access to additional data, what new feature would you collect to strengthen this model?\n",
        "\n",
        "**Answer:** I can collect the percentage of monthly income and expenses to enhance the model."
      ],
      "metadata": {
        "id": "iia0M08ZQevk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8TpGHlgqkf3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fae1baf8"
      },
      "source": [
        "# Task\n",
        "Generate multi-dimensional cross-feature plots using `seaborn.pairplot` to visualize relationships between 'FICO_score', 'Granted_Loan_Amount', 'Monthly_Gross_Income', and 'Monthly_Housing_Payment' with 'Approved' status as hue from the `df_clean` DataFrame. Additionally, create a grouped bar chart to explore the interaction between 'Fico_Score_group' and 'Lender' on loan approval rates using `df_clean`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecff00eb"
      },
      "source": [
        "## generate_cross_feature_plots\n",
        "\n",
        "### Subtask:\n",
        "Generate multi-dimensional cross-feature plots using `seaborn.pairplot` to visualize relationships between key numerical features (FICO_score, Granted_Loan_Amount, Monthly_Gross_Income, Monthly_Housing_Payment) with 'Approved' status as hue. Additionally, create a grouped bar chart to explore the interaction between 'Fico_Score_group' and 'Lender' on loan approval rates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4636c21a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `seaborn.pairplot` visualization was designed to reveal the distributions and pairwise correlations among `FICO_score`, `Granted_Loan_Amount`, `Monthly_Gross_Income`, and `Monthly_Housing_Payment`, differentiated by `Approved` status. This would highlight potential distinct patterns in these financial attributes for approved versus rejected loan applications.\n",
        "*   The grouped bar chart was set up to illustrate loan approval rates across various `Fico_Score_group` categories, further segmented by `Lender`. This analysis would identify variations in lending behavior and approval rates among different lenders based on applicants' FICO scores.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Analyzing these plots could provide insights into the key financial indicators and their thresholds that drive loan approval decisions, as well as how different lenders apply these criteria across various FICO score segments.\n",
        "*   Further investigation could involve quantifying the identified relationships and discrepancies (e.g., specific FICO score cutoffs, income requirements, or lender-specific approval rates), potentially leading to a deeper understanding of loan eligibility criteria and lender policies.\n"
      ]
    }
  ]
}